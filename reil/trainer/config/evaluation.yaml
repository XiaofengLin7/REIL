defaults:
  # - reil_trainer
  - envs
  - _self_

env:
  name: sokoban
  dim_x: 6
  dim_y: 6
  num_boxes: 1
  max_steps: 100
  search_depth: 30 # this will change the starting position of the player

data:
  max_prompt_length: 1024
  max_response_length: 1024

agent_proxy:
  max_turn: 30
  enable_think: False # False -> no think RL(alfworld), True -> think RL(sokoban)

evaluator:
  n_gpus_per_node: 2
  nnodes: 1
  project_name: reil
  experiment_name: eval_ckpts
  logger: [ 'console', 'wandb' ]
  checkpoint_dir: /usr3/graduate/xfl/lab/REIL/checkpoints/ds543/REIL/sokoban-1.5b-0.0075beta-0.001kl-2025-05-01
  resume_step: 0

actor_rollout_ref:
  model:
    path: /usr3/graduate/xfl/lab/REIL/checkpoints/ds543/REIL/alfworld-1.5b-0.005beta-0.001kl-pick_n_place_05-07/huggingface
  actor:
    strategy: fsdp  # This is for backward-compatibility
    ppo_mini_batch_size: 256
    ppo_micro_batch_size: null # will be deprecated, use ppo_micro_batch_size_per_gpu
    ppo_micro_batch_size_per_gpu: null
    use_dynamic_bsz: False
    ppo_max_token_len_per_gpu: 16384 # n * ${data.max_prompt_length} + ${data.max_response_length}
    grad_clip: 1.0
    # pg_losses2 = -advantages * torch.clamp(ratio, 1 - cliprange_low, 1 + cliprange_high)
    clip_ratio: 0.2 # default value if clip_ratio_low and clip_ratio_high are not specified
    clip_ratio_low: 0.2
    clip_ratio_high: 0.2
    clip_ratio_c: 3.0 # lower bound of the value for Dual-clip PPO from https://arxiv.org/pdf/1912.09729
    loss_agg_mode: "token-mean" # / "seq-mean-token-sum" / "seq-mean-token-mean"
    entropy_coeff: 0.001
    use_kl_loss: False # True for GRPO
    use_torch_compile: True # False to disable torch compile
    kl_loss_coef: 0.001 # for grpo
    kl_loss_type: low_var_kl # for grpo
    ppo_epochs: 1
    shuffle: False
    ulysses_sequence_parallel_size: 1 # sp size
    checkpoint:
      contents: ['model', 'optimizer', 'extra']  # with 'hf_model' you can save whole model as hf format, now only use sharded model checkpoint to save space
    optim:
      lr: 1e-6
      lr_warmup_steps: -1 # Prioritized. Negative values mean delegating to lr_warmup_steps_ratio.
      lr_warmup_steps_ratio: 0.  # the total steps will be injected during runtime
      min_lr_ratio: null   # only useful for warmup with cosine
      warmup_style: constant  # select from constant/cosine
      total_training_steps: -1  # must be override by program
      weight_decay: 0.01
    fsdp_config:
      wrap_policy:
        # transformer_layer_cls_to_wrap: None
        min_num_params: 0
      param_offload: False
      optimizer_offload: False
      fsdp_size: -1
    state_masking: False
  rollout:
    name: vllm
    temperature: 1.0
    top_k: -1 # 0 for hf rollout, -1 for vllm rollout
    top_p: 1
    use_fire_sampling: False # https://arxiv.org/abs/2410.21236
    prompt_length: ${data.max_prompt_length}  # not use for opensource
    response_length: ${data.max_response_length}
    # for vllm rollout
    dtype: bfloat16 # should align with FSDP
    gpu_memory_utilization: 0.5
    ignore_eos: False
    enforce_eager: True
    free_cache_engine: True
    load_format: dummy_dtensor
    tensor_model_parallel_size: 1
    max_num_batched_tokens: 8192
    max_model_len: null
    max_num_seqs: 1024
    log_prob_micro_batch_size: null # will be deprecated, use log_prob_micro_batch_size_per_gpu
    log_prob_micro_batch_size_per_gpu: null
    disable_log_stats: True
    enable_chunked_prefill: True # may get higher throughput when set to True. When activated, Please increase max_num_batched_tokens or decrease max_model_len.
    # for hf rollout
    do_sample: True
    # number of responses (i.e. num sample times)
    n: 1 # > 1 for grpo
    val_kwargs:
      # sampling parameters for validation
      top_k: -1 # 0 for hf rollout, -1 for vllm rollout
      top_p: 1.0
      temperature: 0.5
      n: 1
      do_sample: True # default eager for validation

es_manager:
  format_penalty: -0.1
  train:
    env_groups: 8
    # under the same group, the env config and env seed are ensured to be equal
    group_size: 16
    env_configs:
      tags: ["LargerSokoban"]
      n_groups: [8] # If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation
  # val:
  #   env_groups: 256 
  #   group_size: 1 # should be set to 1 because val temperature is set to 0 and same prompt leads to same output
  #   env_configs:
  #     tags: ["LargerSokoban"]
  #     n_groups: [256] # TODO: If not set, all env names divide nums equally. Under the same group, the env config
  val:
    env_groups: 59 # 41 for pick_n_place and pick_two_obj_and_place for out of distribution, 59 for in distribution
    group_size: 1 # should be set to 1 because val temperature is set to 0 and same prompt leads to same output
    env_configs:
      tags: ["ALFWorld"]
      n_groups: [59] # TODO: If not set, all env names divide nums equally. Under the same group, the env config